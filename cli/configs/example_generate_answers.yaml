cli: generate_answers  # Required (str): which CLI this config is for


model: test  # Required (str)
dataset:
  name: trivia_qa  # Required (str)
  num_questions: 5  # Required (int or "all")
  shuffle: True  # Required (bool)
  seed: 42  # Optional (int or None): seed used to shuffle the dataset, ignored if shuffle is False


generation:
  seed: 1337  # Optional (int): random seed for reproducibility, set before each question
  generations_per_question: 2  # Required (int)
  batchsize_per_pass: 2  # Required (int): max batch size per model forward pass
  config:  # Optional (dict): overrides for model's GenerationConfig
  # For more details, see: https://huggingface.co/transformers/main_classes/model.html#transformers.generation.GenerationConfig-
    max_new_tokens: 16
    do_sample: True
    temperature: 1
    eos_token_id: 198  # Stop generating when a gpt2's newline is generated
    pad_token_id: 50256


results:
  dir:
    local: ./results  # Required (dir) if s3 is not specified, otherwise optional
    # s3: s3://my-bucket/results  # Required (s3 url) if local is not specified, otherwise optional
  files:
    text_generations: text_generations.csv  # Required (str)
    qa_pairs: qa_pairs.csv  # Required (str)
  save_frequency: 3  # Optional (int): save results every N questions, otherwise only save at the end


# Required (str): must include a "{}" to replace with a question
# Newlines and trailing whitespaces are preserved
# Left side identation, and end of string newline are ignored
# See here: https://yaml-multiline.info/
prompt_template: |-
  Question: {}
  Answer:
# End of YAML file